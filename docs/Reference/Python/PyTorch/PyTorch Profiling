Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU.

Find the Pytorch Profiler doc at [PyTorch Profiler](https://pytorch-lightning.readthedocs.io/en/stable/profiler.html)

trainer = Trainer(..., profiler="pytorch")

or

profiler = PyTorchProfiler(...)
trainer = Trainer(..., profiler=profiler)
This profiler works with PyTorch DistributedDataParallel. If output_filename is provided, each rank will save their profiled operation to their own file.

The profiler’s results will be printed on the completion of a training fit(). This profiler report can be quite long, so you can also specify an output_filename to save the report instead of logging it to the output in your terminal.

This profiler will record only for training_step_and_backward, evaluation_step and test_step functions by default. The output below shows the profiling for the action training_step_and_backward. The user can provide PyTorchProfiler(profiled_functions=[...]) to extend the scope of profiled functions.

NOTE

When using the PyTorch Profiler, wall clock time will not not be representative of the true wall clock time. This is due to forcing profiled operations to be measured synchronously, when many CUDA ops happen asynchronously. It is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use the SimpleProfiler. # noqa E501

Profiler Report

Profile stats for: training_step_and_backward
---------------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg
---------------------  ---------------  ---------------  ---------------  ---------------  ---------------
t                      62.10%           1.044ms          62.77%           1.055ms          1.055ms
addmm                  32.32%           543.135us        32.69%           549.362us        549.362us
mse_loss               1.35%            22.657us         3.58%            60.105us         60.105us
mean                   0.22%            3.694us          2.05%            34.523us         34.523us
div_                   0.64%            10.756us         1.90%            32.001us         16.000us
ones_like              0.21%            3.461us          0.81%            13.669us         13.669us
sum_out                0.45%            7.638us          0.74%            12.432us         12.432us
transpose              0.23%            3.786us          0.68%            11.393us         11.393us
as_strided             0.60%            10.060us         0.60%            10.060us         3.353us
to                     0.18%            3.059us          0.44%            7.464us          7.464us
empty_like             0.14%            2.387us          0.41%            6.859us          6.859us
empty_strided          0.38%            6.351us          0.38%            6.351us          3.175us
fill_                  0.28%            4.782us          0.33%            5.566us          2.783us
expand                 0.20%            3.336us          0.28%            4.743us          4.743us
empty                  0.27%            4.456us          0.27%            4.456us          2.228us
copy_                  0.15%            2.526us          0.15%            2.526us          2.526us
broadcast_tensors      0.15%            2.492us          0.15%            2.492us          2.492us
size                   0.06%            0.967us          0.06%            0.967us          0.484us
is_complex             0.06%            0.961us          0.06%            0.961us          0.481us
stride                 0.03%            0.517us          0.03%            0.517us          0.517us
---------------------  ---------------  ---------------  ---------------  ---------------  ---------------
Self CPU time total: 1.681ms
When running with PyTorchProfiler(emit_nvtx=True). You should run as following:

nvprof --profile-from-start off -o trace_name.prof -- <regular command here>
To visualize the profiled operation, you can either:

Use:

nvvp trace_name.prof
Use:

python -c 'import torch; print(torch.autograd.profiler.load_nvprof("trace_name.prof"))'
CLASSpytorch_lightning.profiler.AdvancedProfiler(output_filename=None, line_count_restriction=1.0)[SOURCE]
Bases: pytorch_lightning.profiler.profilers.BaseProfiler

This profiler uses Python’s cProfiler to record more detailed information about time spent in each function call recorded during a given action. The output is quite verbose and you should only use this if you want very detailed reports.

Parameters
output_filename (Optional[str]) – optionally save profile results to file instead of printing to std out when training is finished.

line_count_restriction (float) – this can be used to limit the number of functions reported for each action. either an integer (to select a count of lines), or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)

Raises
ValueError – If you attempt to stop recording an action which was never started.

describe()[SOURCE]
Logs a profile report after the conclusion of the training run.

start(action_name)[SOURCE]
Defines how to start recording an action.

Return type
None

stop(action_name)[SOURCE]
Defines how to record the duration once an action is complete.

Return type
None

summary()[SOURCE]
Create profiler summary in text format.

Return type
str

CLASSpytorch_lightning.profiler.BaseProfiler(output_streams=None)[SOURCE]
Bases: abc.ABC

If you wish to write a custom profiler, you should inhereit from this class.

Parameters
output_streams (Union[list, tuple, None]) – callable

describe()[SOURCE]
Logs a profile report after the conclusion of the training run.

Return type
None

profile(action_name)[SOURCE]
Yields a context manager to encapsulate the scope of a profiled action.

Example:

with self.profile('load training data'):
    # load training data code
The profiler will start once you’ve entered the context and will automatically stop once you exit the code block.

Return type
None

ABSTRACT start(action_name)[SOURCE]
Defines how to start recording an action.

Return type
None

ABSTRACT stop(action_name)[SOURCE]
Defines how to record the duration once an action is complete.

Return type
None

ABSTRACT summary()[SOURCE]
Create profiler summary in text format.

Return type
str

CLASSpytorch_lightning.profiler.PassThroughProfiler[SOURCE]
Bases: pytorch_lightning.profiler.profilers.BaseProfiler

This class should be used when you don’t want the (small) overhead of profiling. The Trainer uses this class by default.

Args: output_streams: callable

start(action_name)[SOURCE]
Defines how to start recording an action.

Return type
None

stop(action_name)[SOURCE]
Defines how to record the duration once an action is complete.

Return type
None

summary()[SOURCE]
Create profiler summary in text format.

Return type
str

CLASSpytorch_lightning.profiler.PyTorchProfiler(output_filename=None, enabled=True, use_cuda=False, record_shapes=False, profile_memory=False, group_by_input_shapes=False, with_stack=False, use_kineto=False, use_cpu=True, emit_nvtx=False, export_to_chrome=False, path_to_export_trace=None, row_limit=20, sort_by_key=None, profiled_functions=None, local_rank=None)[SOURCE]
Bases: pytorch_lightning.profiler.profilers.BaseProfiler

This profiler uses PyTorch’s Autograd Profiler and lets you inspect the cost of different operators inside your model - both on the CPU and GPU

Parameters
output_filename (Optional[str]) – optionally save profile results to file instead of printing to std out when training is finished. When using ddp, each rank will stream the profiled operation to their own file with the extension _{rank}.txt

enabled (bool) – Setting this to False makes this context manager a no-op.

use_cuda (bool) – Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.

record_shapes (bool) – If shapes recording is set, information about input dimensions will be collected.

profile_memory (bool) – Whether to report memory usage, default: True (Introduced in PyTorch 1.6.0)

group_by_input_shapes (bool) – Include operator input shapes and group calls by shape.

with_stack (bool) – record source information (file and line number) for the ops (Introduced in PyTorch 1.7.0)

use_kineto (bool) – experimental support for Kineto profiler (Introduced in PyTorch 1.8.0)

use_cpu (bool) – use_kineto=True and can be used to lower the overhead for GPU-only profiling (Introduced in PyTorch 1.8.0)

emit_nvtx (bool) –

Context manager that makes every autograd operation emit an NVTX range Run:

nvprof --profile-from-start off -o trace_name.prof -- <regular command here>


To visualize, you can either use:

nvvp trace_name.prof
torch.autograd.profiler.load_nvprof(path)
export_to_chrome (bool) – Wether to export the sequence of profiled operators for Chrome. It will generate a .json file which can be read by Chrome.

path_to_export_trace (Optional[str]) – Directory path to export .json traces when using export_to_chrome=True. By default, it will be save where the file being is being run.

row_limit (int) – Limit the number of rows in a table, 0 is a special value that removes the limit completely.

sort_by_key (Optional[str]) – Keys to sort out profiled table

profiled_functions (Optional[List]) – list of profiled functions which will create a context manager on. Any other will be pass through.

local_rank (Optional[int]) – When running in distributed setting, local_rank is used for each process to write to their own file if output_fname is provided.

Raises
MisconfigurationException – If arg sort_by_key is not present in AVAILABLE_SORT_KEYS, or if log file is not a .txt file.

ValueError – If you attempt to stop recording an action which was never started.

describe()[SOURCE]
Logs a profile report after the conclusion of the training run.

start(action_name)[SOURCE]
Defines how to start recording an action.

Return type
None

stop(action_name)[SOURCE]
Defines how to record the duration once an action is complete.

Return type
None

summary()[SOURCE]
Create profiler summary in text format.

Return type
str

CLASSpytorch_lightning.profiler.SimpleProfiler(output_filename=None, extended=True)[SOURCE]
Bases: pytorch_lightning.profiler.profilers.BaseProfiler

This profiler simply records the duration of actions (in seconds) and reports the mean duration of each action and the total time spent over the entire training run.

Parameters
output_filename (Optional[str]) – optionally save profile results to file instead of printing to std out when training is finished.

Raises
ValueError – If you attempt to start an action which has already started, or if you attempt to stop recording an action which was never started.

describe()[SOURCE]
Logs a profile report after the conclusion of the training run.

start(action_name)[SOURCE]
Defines how to start recording an action.

Return type
None

stop(action_name)[SOURCE]
Defines how to record the duration once an action is complete.

Return type
None

summary()[SOURCE]
Create profiler summary in text format.

Return type
str

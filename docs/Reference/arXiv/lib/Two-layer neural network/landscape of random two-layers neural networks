For a smooth ψ, we have
∇f(x) = 1
√
k
X
k
`=1
a`w`ψ
0
(w`
· x),
and
∇2
f(x) = 1
√
k
X
k
`=1
a`w`w
>
` ψ
00(w`
· x).

We already claimed in the introduction that, with high probability,
|f(x)| = O(1). (2)
We alluded to the CLT for this claim, but it is also easy to guess it intuitively by noting
that (since E[a`a`
0] = 1{` = `
0}):
E[f(x)
2
] = E
"
1
k
X
k
`,`0=1
a`a`
0ψ(w`
· x)ψ(w`
0 · x)
#
= EX∼N (0,1)[ψ(X)
2
] .
The formal proof of (2) (and all other claims we make here) will eventually be a simple
application of the classical Bernstein concentration inequality. Similarly, it is easy to see
that (note for example that E[k∇f(x)k
2
] = EX∼N (0,1)[ψ
0
(X)
2
]), with high probability,
k∇f(x)k = Θ(1). (3)
A slightly more difficult calculation, although classical too, is that
k∇2
f(x)kop = Oe

1
√
d

. (4)
Indeed one can simply note that, for any u ∈ S
d−1
, u
>∇2
f(x)u = √
1
k
Pk
`=1 a`(w`
·u)
2ψ
00(w`
·x)
is approximately distributed as a centered Gaussian with variance
EX,Y ∼N (0,1):E[XY ]=x·u
"
X
√
d
4
ψ
00 
Y
√
d
2
#
,
so that with probability at least 1 − γ one can expect u
>∇2
f(x)u to be of order
√
log(1/γ)
d
,
and thus by taking a union bound over a discretization of the sphere S
d−1
, one expects the
inequality (4). In fact, interestingly, one can even hope that (4) holds true for an entire ball
around x: with appropriate smoothness over ψ, this could be obtained by doing another
union bound over a second discretization of a d-dimensional ball. In other words, we can
expect that with high probability:
∀x ∈ R
d
: kxk = poly(d), one has k∇2
f(x)kop = Oe

1
√
d

.

The equations (2), (3), and (5) paint a rather clear geometric picture. There are essen-
tially two scales around a fixed x ∈
√
d · S
d−1
: The macroscopic scale, where one considers
a perturbation x + δ with kδk = Ω(√
d), and the mesoscopic scale where kδk = o(
√
d) (we
use this term because for the ReLU network there will also be a microscopic scale, with
kδk = o(1)). At the macroscopic scale the landscape of f might be very complicated, but
our crucial observation is that the picture at the mesoscopic scale is dramatically simpler.
Namely at the mesoscopic scale the function f is essentially linear, since one has (thanks to
(3) and (5))
k∇f(x) − ∇f(x + δ)k = o(k∇f(x)k), ∀δ : kδk = o(
√
d). (6)

Moreover, since the height of the function is constant (by (2)) and the norm of the gradient
is constant, it suffices to step at a constant distance in the direction of the gradient (or
negative gradient) to change the sign of f. In other words, this already proves our main
point: a single step of gradient descent (or ascent) suffices to find an adversarial example,
and moreover the adversarial perturbation δ satisfies kδk = O(1) = O(kxk/
√
d). Formally
one easily concludes from (2), (3), and (6) by using the following simple lemma for gradient
descent:
Lemma 1 For any continuous and almost everywhere differentiable function f, and any
x ∈ R
d and η ∈ R, one has:




f

x +
η
k∇f(x)k
2 ∇f(x)

− (f(x) + η)




≤ sup
δ∈Rd:kδk≤ η
k∇f(x)k
|η|
k∇f(x) − ∇f(x + δ)k
k∇f(x)k
.
Proof. Let g(t) = f

x + t
η
k∇f(x)k
2 ∇f(x)

so that
g
0
(t) = η
k∇f(x)k
2 ∇f(x) · ∇f

x + t
η
k∇f(x)k
2 ∇f(x)

= η + η
∇f(x)
k∇f(x)k
·
∇f

x + t
η
k∇f(x)k
2 ∇f(x)

− ∇f(x)
k∇f(x)k
.
Thus we have:
|g(1) − g(0) − η| ≤ Z 1
0
|g
0
(t) − η|dt ≤ |η|
Z 1
0





∇f

x + t
η
k∇f(x)k
2 ∇f(x)

− ∇f(x)






k∇f(x)k
dt ,
which concludes the proof.
